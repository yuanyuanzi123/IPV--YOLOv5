# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license
"""
YOLO-specific modules.

Usage:
    $ python models/yolo.py --cfg yolov5s.yaml
"""

import argparse
import contextlib
import math
import os
import platform
import sys
from copy import deepcopy
from pathlib import Path

import torch
import torch.nn as nn

FILE = Path(__file__).resolve()
ROOT = FILE.parents[1]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
if platform.system() != "Windows":
    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

from models.common import (
    SE_Block,
    C3,
    C3SPP,
    C3TR,
    SPP,
    SPPF,
    Bottleneck,
    BottleneckCSP,
    C3Ghost,
    C3x,
    Classify,
    Concat,
    Contract,
    Conv,
    CrossConv,
    DetectMultiBackend,
    DWConv,
    DWConvTranspose2d,
    Expand,
    Focus,
    GhostBottleneck,
    GhostConv,
    Proto,
    FEM,
    PSA,
    SPPFCSPC,
    C2,
    C2f,
    FEMS,
    FFM_Concat2,
    FFM_Concat3,
    C2fCIBAttention,
    SE,
    G_bneck,
    C3_DCN,
    LSKblock,
    CoordAtt,
    C3_CA,
    DSConv,
    DSConv_C3,
    SCAM,
    SCAMS,
    SCAMSE,
)

from models.CSPPC import CSPPC
from models.mobile import MobileViTv2_Block

from models.C2FGhost import C2fGhost

from models.MLLA import MLLAttention
from models.experimental import MixConv2d
from models.SEAttention import SEAttention
from models.ema import EMA
from models.selfattention import SelfAttention
from models.mul import MultiSpectralAttentionLayer
from models.SPPELAN import SPPELAN
from utils.autoanchor import check_anchor_order
from utils.general import LOGGER, check_version, check_yaml, colorstr, make_divisible, print_args
from utils.plots import feature_visualization
from utils.torch_utils import (
    fuse_conv_and_bn,#åˆå¹¶å·ç§¯å±‚å’Œæ‰¹é‡å½’ä¸€åŒ–å±‚ã€‚
    initialize_weights,#åˆå§‹åŒ–æ¨¡å‹çš„æƒé‡ã€‚
    model_info,#è·å–æ¨¡å‹çš„ä¿¡æ¯ï¼Œæ¯”å¦‚å±‚æ¬¡ç»“æ„ã€å‚æ•°æ•°é‡ç­‰
    profile,#å¯¹æ¨¡å‹è¿›è¡Œæ€§èƒ½åˆ†æï¼Œé€šå¸¸æ˜¯ä¸ºäº†æ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆ
    scale_img,#è°ƒæ•´å›¾åƒå¤§å°ã€‚
    select_device,#é€‰æ‹©è¿è¡Œæ¨¡å‹çš„è®¾å¤‡ï¼Œæ¯”å¦‚ CPU æˆ– GPUã€‚
    time_sync,#æ—¶é—´åŒæ­¥ï¼Œå¯èƒ½æ˜¯ç”¨äºåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ä¿æŒæ—¶é—´åŒæ­¥ã€‚
)

try:
    import thop  # for FLOPs computation
except ImportError:  #å¤„ç†å¯¼å…¥å¤±è´¥çš„æƒ…å†µ
    thop = None


class Detect(nn.Module):
    # YOLOv5 Detect head for detection models
    stride = None  # strides computed during build   (ç±»å±æ€§ï¼Œç”¨äºæŒ‡ç¤ºç‰¹å¾å›¾çš„æ­¥å¹…ã€‚)
    dynamic = False  # force grid reconstruction   (ç±»å±æ€§ï¼Œå®ƒç”¨äºæ§åˆ¶æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨åŠ¨æ€ç½‘æ ¼é‡å»ºã€‚)
    export = False  # export mode  ï¼ˆexport æ˜¯ä¸€ä¸ªç±»å±æ€§ï¼Œåˆå§‹åŒ–ä¸º Falseã€‚å®ƒç”¨äºæŒ‡ç¤ºæ¨¡å‹æ˜¯å¦å¤„äºå¯¼å‡ºæ¨¡å¼ã€‚ï¼‰

    def __init__(self, nc=80, anchors=(), ch=(), inplace=True):
        """Initializes YOLOv5 detection layer with specified classes, anchors, channels, and inplace operations."""
        super().__init__()
        self.nc = nc  # number of classes
        self.no = nc + 5  # number of outputs per anchor
        self.nl = len(anchors)  # number of detection layers
        self.na = len(anchors[0]) // 2  # number of anchors
        self.grid = [torch.empty(0) for _ in range(self.nl)]  # init grid
        self.anchor_grid = [torch.empty(0) for _ in range(self.nl)]  # init anchor grid
        self.register_buffer("anchors", torch.tensor(anchors).float().view(self.nl, -1, 2))  # shape(nl,na,2)
        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv
        self.inplace = inplace  # use inplace ops (e.g. slice assignment)

    def forward(self, x):     #è¿™æ®µä»£ç å®Œæˆäº† Detect ç±»çš„å‰å‘ä¼ æ’­æ–¹æ³•çš„å®šä¹‰ï¼Œç”¨äºå¯¹è¾“å…¥æ•°æ®è¿›è¡Œå¤„ç†ï¼Œå¹¶ç”Ÿæˆæ£€æµ‹ç»“æœã€‚
        #è¿™æ˜¯ Detect ç±»çš„å‰å‘ä¼ æ’­æ–¹æ³• forward() çš„å®šä¹‰ï¼Œå®ƒæ¥å—ä¸€ä¸ªå‚æ•° xï¼Œä»£è¡¨è¾“å…¥æ•°æ®
        """Processes input through YOLOv5 layers, altering shape for detection: `x(bs, 3, ny, nx, 85)`."""
        #è¿™æ˜¯å¯¹ forward() æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œè¯´æ˜è¯¥æ–¹æ³•çš„ä½œç”¨å’Œè¾“å…¥æ•°æ®çš„æ ¼å¼
        z = []  # inference output
        #åˆ›å»ºäº†ä¸€ä¸ªç©ºåˆ—è¡¨ zï¼Œç”¨äºå­˜å‚¨æ¨ç†è¾“å‡ºã€‚
        for i in range(self.nl):     #å¯¹æ¯ä¸ªæ£€æµ‹å±‚è¿›è¡Œè¿­ä»£å¤„ç†ã€‚
            x[i] = self.m[i](x[i])  # conv     (å¯¹è¾“å…¥æ•°æ® x ç»è¿‡ç¬¬ i ä¸ªå·ç§¯å±‚è¿›è¡Œå·ç§¯æ“ä½œï¼Œæ›´æ–° x[i])
            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
            #è·å– x[i] çš„å½¢çŠ¶ä¿¡æ¯ï¼Œå…¶ä¸­ bs è¡¨ç¤ºæ‰¹é‡å¤§å°ï¼Œny å’Œ nx åˆ†åˆ«è¡¨ç¤ºç‰¹å¾å›¾çš„é«˜åº¦å’Œå®½åº¦
            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()
                #è°ƒæ•´ x[i] çš„å½¢çŠ¶ï¼Œä»¥ä¾¿åç»­å¤„ç†ã€‚å°†å…¶è§†ä¸ºå½¢çŠ¶ä¸º (bs, na, no, ny, nx) çš„å¼ é‡ï¼Œå¹¶é‡æ–°æ’åˆ—ç»´åº¦é¡ºåº
            if not self.training:  # inference  å¦‚æœæ¨¡å‹å¤„äºæ¨ç†æ¨¡å¼ä¸‹
                if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:
                    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)
                        #å¦‚æœéœ€è¦åŠ¨æ€ç½‘æ ¼é‡å»ºæˆ–è€…ç½‘æ ¼å½¢çŠ¶ä¸è¾“å…¥å¼ é‡ä¸åŒ¹é…ï¼Œåˆ™é‡æ–°åˆ›å»ºç½‘æ ¼ã€‚

                if isinstance(self, Segment):  # (boxes + masks)  å¦‚æœ self ç±»å‹ä¸º Segmentï¼Œå³å«æœ‰åˆ†å‰²åŠŸèƒ½
                    xy, wh, conf, mask = x[i].split((2, 2, self.nc + 1, self.no - self.nc - 5), 4)
                    #å°†è¾“å…¥å¼ é‡ x[i] æ‹†åˆ†ä¸ºåæ ‡ xyã€å®½é«˜ whã€ç½®ä¿¡åº¦ conf å’Œæ©ç  mask
                    xy = (xy.sigmoid() * 2 + self.grid[i]) * self.stride[i]  # xy
                    wh = (wh.sigmoid() * 2) ** 2 * self.anchor_grid[i]  # wh
                    #å¯¹åæ ‡å’Œå®½é«˜è¿›è¡Œè§£ç ï¼Œå¾—åˆ°çœŸå®çš„è¾¹ç•Œæ¡†åæ ‡å’Œå°ºå¯¸
                    y = torch.cat((xy, wh, conf.sigmoid(), mask), 4)
                    #å°†è§£ç åçš„ç»“æœæ‹¼æ¥ä¸ºè¾“å‡ºå¼ é‡ yã€‚
                else:  # Detect (boxes only)    å¦åˆ™ï¼Œå³æ‰§è¡Œç›®æ ‡æ£€æµ‹ä»»åŠ¡
                    xy, wh, conf = x[i].sigmoid().split((2, 2, self.nc + 1), 4)
                    #å°†è¾“å…¥å¼ é‡ x[i] çš„é¢„æµ‹ç»“æœæ‹†åˆ†ä¸ºåæ ‡ xyã€å®½é«˜ wh å’Œç½®ä¿¡åº¦ confã€‚
                    xy = (xy * 2 + self.grid[i]) * self.stride[i]  # xy
                    wh = (wh * 2) ** 2 * self.anchor_grid[i]  # wh
                    #å¯¹åæ ‡å’Œå®½é«˜è¿›è¡Œè§£ç ï¼Œå¾—åˆ°çœŸå®çš„è¾¹ç•Œæ¡†åæ ‡å’Œå°ºå¯¸
                    y = torch.cat((xy, wh, conf), 4)
                    #å°†è§£ç åçš„ç»“æœæ‹¼æ¥ä¸ºè¾“å‡ºå¼ é‡ y
                z.append(y.view(bs, self.na * nx * ny, self.no))
                #å°†è§£ç åçš„ç»“æœæ‹¼æ¥ä¸ºè¾“å‡ºå¼ é‡ y

        return x if self.training else (torch.cat(z, 1),) if self.export else (torch.cat(z, 1), x)
                    #å¦‚æœæ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼ä¸‹ï¼Œè¿”å›å¤„ç†åçš„è¾“å…¥å¼ é‡ xï¼›å¦‚æœå¤„äºå¯¼å‡ºæ¨¡å¼ä¸‹ï¼Œè¿”å›æ¨ç†è¾“å‡ºåˆ—è¡¨ zï¼›å¦åˆ™è¿”å›æ¨ç†è¾“å‡ºåˆ—è¡¨ z å’Œå¤„ç†åçš„è¾“å…¥å¼ é‡ x
    def _make_grid(self, nx=20, ny=20, i=0, torch_1_10=check_version(torch.__version__, "1.10.0")):
        """Generates a mesh grid for anchor boxes with optional compatibility for torch versions < 1.10."""
        d = self.anchors[i].device
        t = self.anchors[i].dtype
        shape = 1, self.na, ny, nx, 2  # grid shape
        y, x = torch.arange(ny, device=d, dtype=t), torch.arange(nx, device=d, dtype=t)
        #ä½¿ç”¨ torch.arange åˆ›å»ºè¡¨ç¤ºæ²¿ y è½´å’Œ x è½´çš„ç´¢å¼•çš„å¼ é‡ y å’Œ x
        yv, xv = torch.meshgrid(y, x, indexing="ij") if torch_1_10 else torch.meshgrid(y, x)  # torch>=0.7 compatibility
        #ä½¿ç”¨ torch.meshgrid æ„é€ ç½‘æ ¼ yv å’Œ xvï¼Œå¦‚æœ Torch ç‰ˆæœ¬ä¸º 1.10 æˆ–æ›´é«˜ï¼Œåˆ™ä½¿ç”¨ 'ij' ç´¢å¼•ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è¡Œä¸º
        grid = torch.stack((xv, yv), 2).expand(shape) - 0.5  # add grid offset, i.e. y = 2.0 * x - 0.5
        #é€šè¿‡å †å  xv å’Œ yv å¹¶æ·»åŠ ç½‘æ ¼åç§»é‡ -0.5 æ¥æ„é€ ç½‘æ ¼å¼ é‡ã€‚
        anchor_grid = (self.anchors[i] * self.stride[i]).view((1, self.na, 1, 1, 2)).expand(shape)
        return grid, anchor_grid
        #å°†é”šæ¡†å¼ é‡ä¹˜ä»¥å…¶ç›¸åº”çš„æ­¥é•¿ï¼Œå¹¶é‡å¡‘ä»¥åŒ¹é…ç½‘æ ¼å¼ é‡çš„å½¢çŠ¶ï¼Œæ„é€ é”šæ¡†ç½‘æ ¼å¼ é‡ã€‚
        #è¿”å›ç½‘æ ¼å¼ é‡å’Œé”šæ¡†ç½‘æ ¼å¼ é‡ä½œä¸ºä¸€ä¸ªå…ƒç»„

        #è¯¥æ–¹æ³•å®è´¨ä¸Šç”Ÿæˆäº†ä¸€ç»„ç½‘æ ¼ç‚¹å’Œç›¸åº”çš„é”šæ¡†ï¼Œè¿™åœ¨ç›®æ ‡æ£€æµ‹ç®—æ³•ï¼ˆå¦‚ YOLO æˆ– SSDï¼‰ä¸­ä½¿ç”¨ã€‚ç½‘æ ¼ç”± nx * ny ä¸ªç‚¹ç»„æˆï¼Œå¯¹äºæ¯ä¸ªç‚¹ï¼Œéƒ½å®šä¹‰äº† self.na ä¸ªé”šæ¡†


class Segment(Detect):
    # YOLOv5 Segment head for segmentation models
    def __init__(self, nc=80, anchors=(), nm=32, npr=256, ch=(), inplace=True):
        #ncï¼šç±»åˆ«æ•°é‡ï¼ˆé»˜è®¤å€¼ä¸º 80ï¼‰ã€‚anchorsï¼šé”šæ¡†ã€‚nmï¼šæ©è†œæ•°é‡ï¼ˆé»˜è®¤å€¼ä¸º 32ï¼‰ã€‚nprï¼šåŸå‹æ•°é‡ï¼ˆé»˜è®¤å€¼ä¸º 256ï¼‰ã€‚chï¼šé€šé“æ•°ã€‚inplaceï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦è¿›è¡ŒåŸåœ°æ“ä½œï¼ˆé»˜è®¤å€¼ä¸º Trueï¼‰
        """Initializes YOLOv5 Segment head with options for mask count, protos, and channel adjustments."""
        super().__init__(nc, anchors, ch, inplace)
        self.nm = nm  # number of masks
        self.npr = npr  # number of protos
        self.no = 5 + nc + self.nm  # number of outputs per anchor
        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv
        #mï¼šæ˜¯ä¸€ä¸ªæ¨¡å—åˆ—è¡¨ï¼Œå…¶ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ª nn.Conv2d å¯¹è±¡ï¼Œç”¨äºè¾“å‡ºé¢„æµ‹ç»“æœã€‚è¿™äº›å·ç§¯å±‚çš„è¾“å…¥é€šé“æ•°åˆ†åˆ«æ¥è‡ªå‚æ•° ch ä¸­çš„æ¯ä¸ªå€¼ï¼Œè¾“å‡ºé€šé“æ•°ä¸º self.no * self.na
        self.proto = Proto(ch[0], self.npr, self.nm)  # protos protoï¼šæ˜¯ä¸€ä¸ª Proto å¯¹è±¡ï¼Œå®ƒçš„åˆå§‹åŒ–å‡½æ•°æ¥å— ch[0]ï¼ˆch ä¸­çš„ç¬¬ä¸€ä¸ªå€¼ï¼‰ã€self.npr å’Œ self.nm ä½œä¸ºå‚æ•°ã€‚
        self.detect = Detect.forward

        #è¿™ä¸ªç±»ä¼¼ä¹æ˜¯ç”¨äºå®ç° YOLOv5 çš„åˆ†å‰²å¤´éƒ¨ï¼Œå®ƒåŒ…æ‹¬äº†ç”¨äºåˆ†å‰²æ¨¡å‹çš„æ©è†œæ•°é‡ã€åŸå‹æ•°é‡å’Œé€šé“è°ƒæ•´ç­‰é€‰é¡¹

    def forward(self, x):
        """Processes input through the network, returning detections and prototypes; adjusts output based on
        training/export mode.
        """
        p = self.proto(x[0])
        x = self.detect(self, x)
        return (x, p) if self.training else (x[0], p) if self.export else (x[0], p, x[1])


class BaseModel(nn.Module):   #å®ƒçš„ forward æ–¹æ³•ç”¨äºæ‰§è¡Œå•å°ºåº¦çš„æ¨ç†æˆ–è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶æ”¯æŒæ€§èƒ½åˆ†æå’Œå¯è§†åŒ–ã€‚
    """YOLOv5 base model."""

    def forward(self, x, profile=False, visualize=False):
        #profileï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦è¿›è¡Œæ€§èƒ½åˆ†æï¼ˆé»˜è®¤ä¸º Falseï¼‰ã€‚visualizeï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦è¿›è¡Œå¯è§†åŒ–ï¼ˆé»˜è®¤ä¸º Falseï¼‰ã€‚
        """Executes a single-scale inference or training pass on the YOLOv5 base model, with options for profiling and
        visualization.
        """
        return self._forward_once(x, profile, visualize)  # single-scale inference, train
        #æ–¹æ³•å†…éƒ¨è°ƒç”¨äº† _forward_once æ–¹æ³•ï¼Œä¼ é€’äº†å‚æ•° xã€profile å’Œ visualizeï¼Œå¹¶è¿”å›å…¶ç»“æœã€‚è¿™è¡¨æ˜ _forward_once æ–¹æ³•å¯èƒ½æ˜¯ç±»ä¸­çš„å¦ä¸€ä¸ªæ–¹æ³•ï¼Œè´Ÿè´£æ‰§è¡Œå•æ¬¡å‰å‘ä¼ æ’­çš„å…·ä½“é€»è¾‘ã€‚
    def _forward_once(self, x, profile=False, visualize=False):
        """Performs a forward pass on the YOLOv5 model, enabling profiling and feature visualization options."""
        y, dt = [], []  # outputs
        for m in self.model:
            if m.f != -1:  # if not from previous layer
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers
            #å¦‚æœ m.f != -1ï¼Œå³ä¸æ˜¯ä»å‰ä¸€å±‚è·å–è¾“å…¥ï¼Œåˆ™æ ¹æ® m.f æ¥ç¡®å®šè¾“å…¥ xã€‚å¦‚æœ m.f æ˜¯æ•´æ•°ï¼Œåˆ™è¡¨ç¤ºä»å…ˆå‰çš„å±‚è·å–è¾“å…¥ï¼›å¦‚æœ m.f æ˜¯åˆ—è¡¨ï¼Œåˆ™æ ¹æ®åˆ—è¡¨ä¸­çš„ç´¢å¼•è·å–è¾“å…¥ã€‚
            if profile:   #å¦‚æœå¼€å¯äº†æ€§èƒ½åˆ†æ (profile)ï¼Œåˆ™è°ƒç”¨ _profile_one_layer æ–¹æ³•æ¥å¯¹å½“å‰å±‚è¿›è¡Œæ€§èƒ½åˆ†æã€‚
                self._profile_one_layer(m, x, dt)
            x = m(x)  # run  æ‰§è¡Œå½“å‰å±‚çš„å‰å‘ä¼ æ’­æ“ä½œï¼Œå°†ç»“æœä¿å­˜åœ¨ x ä¸­ã€‚
            y.append(x if m.i in self.save else None)  # save output
            #å¦‚æœå½“å‰å±‚çš„ç´¢å¼• m.i åœ¨ self.save ä¸­ï¼Œåˆ™å°†å½“å‰å±‚çš„è¾“å‡ºä¿å­˜åˆ°åˆ—è¡¨ y ä¸­ã€‚
            if visualize:   #å¦‚æœå¼€å¯äº†ç‰¹å¾å¯è§†åŒ– (visualize)ï¼Œåˆ™è°ƒç”¨ feature_visualization æ–¹æ³•å¯¹å½“å‰å±‚çš„è¾“å‡ºè¿›è¡Œå¯è§†åŒ–ã€‚
                feature_visualization(x, m.type, m.i, save_dir=visualize)
        return x

    def _profile_one_layer(self, m, x, dt):   #è¿™ä¸ª _profile_one_layer æ–¹æ³•ç”¨äºå¯¹å•ä¸ªå±‚çš„æ€§èƒ½è¿›è¡Œåˆ†æï¼ŒåŒ…æ‹¬è®¡ç®— GFLOPsï¼ˆåäº¿æ¬¡æµ®ç‚¹è¿ç®—ï¼‰ã€æ‰§è¡Œæ—¶é—´å’Œå‚æ•°æ•°é‡
        """Profiles a single layer's performance by computing GFLOPs, execution time, and parameters."""
        c = m == self.model[-1]  # is final layer, copy input as inplace fix
        #é¦–å…ˆï¼Œæ–¹æ³•æ£€æŸ¥å½“å‰å±‚æ˜¯å¦æ˜¯æ¨¡å‹çš„æœ€åä¸€å±‚ï¼ˆé€šè¿‡åˆ¤æ–­ m == self.model[-1]ï¼‰ã€‚å¦‚æœæ˜¯æœ€åä¸€å±‚ï¼Œåˆ™å°†è¾“å…¥æ•°æ®è¿›è¡Œå¤åˆ¶ï¼Œä»¥ä¿®å¤å°±åœ°æ“ä½œçš„é—®é¢˜ï¼ˆinplace fixï¼‰
        o = thop.profile(m, inputs=(x.copy() if c else x,), verbose=False)[0] / 1e9 * 2 if thop else 0  # FLOPs
        #æ–¹æ³•ä½¿ç”¨ thop åº“æ¥è®¡ç®—å½“å‰å±‚çš„ GFLOPsã€‚å¦‚æœ thop ä¸å¯ç”¨ï¼Œåˆ™å°† GFLOPs è®¾ä¸º 0ã€‚
        t = time_sync()
        #æ–¹æ³•é€šè¿‡è°ƒç”¨ time_sync() å‡½æ•°è®¡ç®—å½“å‰å±‚çš„æ‰§è¡Œæ—¶é—´ã€‚åœ¨è¿™é‡Œï¼Œä½¿ç”¨äº†ä¸€ä¸ªå¾ªç¯æ¥å¤šæ¬¡è¿è¡Œå½“å‰å±‚ï¼Œä»¥è·å¾—æ›´å‡†ç¡®çš„æ‰§è¡Œæ—¶é—´ã€‚è®¡ç®—å¾—åˆ°çš„æ‰§è¡Œæ—¶é—´å•ä½ä¸ºæ¯«ç§’ã€‚
        for _ in range(10):
            m(x.copy() if c else x)
        dt.append((time_sync() - t) * 100)
        if m == self.model[0]:
            LOGGER.info(f"{'time (ms)':>10s} {'GFLOPs':>10s} {'params':>10s}  module")
        LOGGER.info(f"{dt[-1]:10.2f} {o:10.2f} {m.np:10.0f}  {m.type}")
        #æ–¹æ³•è¾“å‡ºå½“å‰å±‚çš„æ‰§è¡Œæ—¶é—´ã€GFLOPs å’Œå‚æ•°æ•°é‡ã€‚å¦‚æœå½“å‰å±‚æ˜¯æ¨¡å‹çš„ç¬¬ä¸€å±‚ï¼Œåˆ™è¾“å‡ºä¸€ä¸ªè¡¨å¤´ã€‚
        if c:
            LOGGER.info(f"{sum(dt):10.2f} {'-':>10s} {'-':>10s}  Total")
            #å¦‚æœå½“å‰å±‚æ˜¯æ¨¡å‹çš„æœ€åä¸€å±‚ï¼Œæ–¹æ³•è¿˜ä¼šè¾“å‡ºæ€»çš„æ‰§è¡Œæ—¶é—´ï¼Œä½†ä¸ä¼šè¾“å‡º GFLOPs å’Œå‚æ•°æ•°é‡ã€‚


            #è¿™ä¸ªæ–¹æ³•ç”¨äºå¯¹å•ä¸ªå±‚çš„æ€§èƒ½è¿›è¡Œåˆ†æï¼Œå¹¶è¾“å‡ºæ‰§è¡Œæ—¶é—´ã€GFLOPs å’Œå‚æ•°æ•°é‡ã€‚

    def fuse(self):  #è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º fuse çš„æ–¹æ³•ï¼Œç”¨äºåœ¨æ¨¡å‹ä¸­èåˆ Conv2d() å’Œ BatchNorm2d() å±‚ï¼Œä»¥æé«˜æ¨ç†é€Ÿåº¦ã€‚
        """Fuses Conv2d() and BatchNorm2d() layers in the model to improve inference speed."""
        LOGGER.info("Fusing layers... ")
        for m in self.model.modules():  #æ–¹æ³•å†…éƒ¨éå†æ¨¡å‹ä¸­çš„æ¯ä¸ªæ¨¡å—ï¼Œé€šè¿‡ self.model.modules() è·å–æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—ã€‚
            if isinstance(m, (Conv, DWConv)) and hasattr(m, "bn"):
                #å¯¹äºæ¯ä¸ªæ¨¡å— mï¼Œæ£€æŸ¥å®ƒæ˜¯å¦æ˜¯ Conv æˆ– DWConv ç±»å‹ï¼Œå¹¶ä¸”å…·æœ‰å±æ€§ bnã€‚Conv å’Œ DWConv åº”è¯¥æ˜¯æŸäº›ç‰¹å®šç±»å‹çš„å·ç§¯å±‚
                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv
                #è°ƒç”¨ fuse_conv_and_bn å‡½æ•°æ¥èåˆå·ç§¯å±‚å’Œæ‰¹æ ‡å‡†åŒ–å±‚ï¼Œæ›´æ–°å·ç§¯å±‚ m.conv
                delattr(m, "bn")  # remove batchnorm   åˆ é™¤æ¨¡å— m çš„ bn å±æ€§ï¼Œä»¥ç§»é™¤æ‰¹æ ‡å‡†åŒ–å±‚
                m.forward = m.forward_fuse  # update forward
                #æ›´æ–°æ¨¡å— m çš„ forward æ–¹æ³•ä¸º m.forward_fuseï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªæ›¿ä»£çš„å‰å‘ä¼ æ’­æ–¹æ³•ï¼Œç”¨äºåœ¨èåˆåçš„æ¨¡å—ä¸­ä½¿ç”¨ã€‚
        self.info()  #åœ¨éå†å®Œæˆåï¼Œè¾“å‡ºä¿¡æ¯ä»¥æ˜¾ç¤ºèåˆåçš„æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¹¶è¿”å› selfï¼Œä»¥ä¾¿å¯ä»¥è¿ç»­è°ƒç”¨å…¶ä»–æ–¹æ³•ã€‚
        return self  #è¿™ä¸ªæ–¹æ³•ç”¨äºåœ¨æ¨¡å‹ä¸­èåˆå·ç§¯å±‚å’Œæ‰¹æ ‡å‡†åŒ–å±‚ï¼Œä»¥æé«˜æ¨ç†é€Ÿåº¦ã€‚

    def info(self, verbose=False, img_size=640):  #è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º info çš„æ–¹æ³•ï¼Œç”¨äºæ‰“å°æ¨¡å‹çš„ä¿¡æ¯ï¼Œæ ¹æ® verbosityï¼ˆè¯¦ç»†ç¨‹åº¦ï¼‰å’Œå›¾åƒå¤§å°è¿›è¡Œè°ƒæ•´
        #verboseï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼ˆé»˜è®¤ä¸º Falseï¼‰img_sizeï¼šä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºå›¾åƒçš„å¤§å°ï¼ˆé»˜è®¤ä¸º 640ï¼‰ã€‚
        """Prints model information given verbosity and image size, e.g., `info(verbose=True, img_size=640)`."""
        model_info(self, verbose, img_size)
        #æ–¹æ³•è°ƒç”¨äº†ä¸€ä¸ªåä¸º model_info çš„å‡½æ•°ï¼Œä¼ é€’äº†æ¨¡å‹è‡ªèº« selfã€verbose å’Œ img_size ä½œä¸ºå‚æ•°ã€‚è¿™ä¸ªå‡½æ•°å¯èƒ½æ˜¯ä¸€ä¸ªå¤–éƒ¨å®šä¹‰çš„å‡½æ•°ï¼Œç”¨äºæ‰“å°æ¨¡å‹çš„ä¿¡æ¯ã€‚
        #åœ¨ model_info å‡½æ•°å†…éƒ¨ï¼Œæ ¹æ®ä¼ å…¥çš„å‚æ•°ï¼Œæ‰“å°äº†æ¨¡å‹çš„ä¿¡æ¯ã€‚å¯èƒ½ä¼šåŒ…æ‹¬æ¨¡å‹çš„å±‚æ¬¡ç»“æ„ã€å‚æ•°æ•°é‡ã€æ¨ç†é€Ÿåº¦ç­‰ã€‚è¿™ä¸ªæ–¹æ³•ç”¨äºæ‰“å°æ¨¡å‹çš„ä¿¡æ¯ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è®¾ç½®è¯¦ç»†ç¨‹åº¦å’Œå›¾åƒå¤§å°

    def _apply(self, fn):
        #è¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º _apply çš„æ–¹æ³•ï¼Œç”¨äºå¯¹æ¨¡å‹å¼ é‡åº”ç”¨è½¬æ¢ï¼Œä¾‹å¦‚ to()ã€cpu()ã€cuda()ã€half()ï¼Œä½†ä¸åŒ…æ‹¬å‚æ•°æˆ–å·²æ³¨å†Œçš„ç¼“å†²åŒº
        """Applies transformations like to(), cpu(), cuda(), half() to model tensors excluding parameters or registered
        buffers.
        """
        self = super()._apply(fn) #æ–¹æ³•æ¥å—ä¸€ä¸ªå‚æ•° fnï¼Œä»£è¡¨äº†åº”ç”¨çš„è½¬æ¢å‡½æ•°ã€‚
        #æ–¹æ³•è°ƒç”¨äº†çˆ¶ç±»çš„ _apply æ–¹æ³•ï¼Œå¹¶å°†è¿”å›ç»“æœèµ‹å€¼ç»™ selfï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„å¼ é‡éƒ½åº”ç”¨äº†ç›¸åŒçš„è½¬æ¢ã€‚
        m = self.model[-1]  # Detect()
        #æ–¹æ³•è·å–æ¨¡å‹çš„æœ€åä¸€ä¸ªæ¨¡å— mï¼Œé€šå¸¸æ˜¯ Detect() æˆ– Segment() ç±»å‹ã€‚
        if isinstance(m, (Detect, Segment)):#å¦‚æœ m æ˜¯ Detect() æˆ– Segment() ç±»å‹çš„å®ä¾‹ï¼Œå³ä¸ºæ£€æµ‹æˆ–åˆ†å‰²æ¨¡å‹ï¼š
            m.stride = fn(m.stride) #å°† m çš„æ­¥é•¿ï¼ˆstrideï¼‰åº”ç”¨è½¬æ¢å‡½æ•° fn
            m.grid = list(map(fn, m.grid)) #å¯¹ m çš„ç½‘æ ¼ï¼ˆgridï¼‰ä¸­çš„æ¯ä¸ªå¼ é‡éƒ½åº”ç”¨è½¬æ¢å‡½æ•° fn
            if isinstance(m.anchor_grid, list):
                m.anchor_grid = list(map(fn, m.anchor_grid))  #å¦‚æœ m çš„é”šæ¡†ç½‘æ ¼ï¼ˆanchor_gridï¼‰æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œä¹Ÿå¯¹å…¶ä¸­çš„æ¯ä¸ªå¼ é‡åº”ç”¨è½¬æ¢å‡½æ•° fnã€‚
        return self   #è¿™ä¸ªæ–¹æ³•ç”¨äºå¯¹æ¨¡å‹çš„å¼ é‡åº”ç”¨è½¬æ¢ï¼Œä½†ä¸åŒ…æ‹¬æ¨¡å‹çš„å‚æ•°æˆ–å·²æ³¨å†Œçš„ç¼“å†²åŒºã€‚


class DetectionModel(BaseModel):
    # YOLOv5 detection model
    def __init__(self, cfg="yolov5s.yaml", ch=3, nc=None, anchors=None):
        """Initializes YOLOv5 model with configuration file, input channels, number of classes, and custom anchors."""
        super().__init__()
        if isinstance(cfg, dict):   #å¦‚æœ cfg æ˜¯å­—å…¸ç±»å‹ï¼Œåˆ™ç›´æ¥å°†å…¶ä½œä¸ºæ¨¡å‹çš„é…ç½®ä¿¡æ¯
            self.yaml = cfg  # model dict   å¦‚æœ cfg æ˜¯ .yaml æ–‡ä»¶è·¯å¾„ï¼Œåˆ™ä½¿ç”¨ PyYAML åº“åŠ è½½è¯¥æ–‡ä»¶å¹¶å°†å…¶ä½œä¸ºæ¨¡å‹çš„é…ç½®ä¿¡æ¯ã€‚
        else:  # is *.yaml
            import yaml  # for torch hub

            self.yaml_file = Path(cfg).name
            with open(cfg, encoding="ascii", errors="ignore") as f:
                self.yaml = yaml.safe_load(f)  # model dict

        # Define model
        ch = self.yaml["ch"] = self.yaml.get("ch", ch)  # input channels è·å–é…ç½®æ–‡ä»¶ä¸­çš„è¾“å…¥é€šé“æ•° chï¼Œå¹¶è¦†ç›–é»˜è®¤å€¼ã€‚
        if nc and nc != self.yaml["nc"]: #å¦‚æœä¼ å…¥äº†ç±»åˆ«æ•° ncï¼Œåˆ™å°†é…ç½®æ–‡ä»¶ä¸­çš„ç±»åˆ«æ•°è¦†ç›–ä¸ºä¼ å…¥å€¼ã€‚
            LOGGER.info(f"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}")
            self.yaml["nc"] = nc  # override yaml value
        if anchors:
            LOGGER.info(f"Overriding model.yaml anchors with anchors={anchors}")
            self.yaml["anchors"] = round(anchors)  # override yaml value å¦‚æœä¼ å…¥äº†è‡ªå®šä¹‰é”šæ¡† anchorsï¼Œåˆ™ä½¿ç”¨ä¼ å…¥å€¼è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„é”šæ¡†ä¿¡æ¯ã€‚
        self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist
        #è§£ææ¨¡å‹ç»“æ„ï¼Œå¹¶æ ¹æ®è¾“å…¥é€šé“æ•°æ„å»ºæ¨¡å‹ã€‚
        self.names = [str(i) for i in range(self.yaml["nc"])]  # default names
        self.inplace = self.yaml.get("inplace", True)

        # Build strides, anchors
        m = self.model[-1]  # Detect()   æ ¹æ®æ¨¡å‹çš„æœ€åä¸€å±‚ç±»å‹ï¼ˆDetect æˆ– Segmentï¼‰ï¼Œè®¾ç½®æ¨¡å‹çš„æ­¥é•¿ï¼ˆstrideï¼‰å’Œé”šæ¡†ï¼ˆanchorsï¼‰ã€‚
        if isinstance(m, (Detect, Segment)):
            s = 256  # 2x min stride
            m.inplace = self.inplace
            forward = lambda x: self.forward(x)[0] if isinstance(m, Segment) else self.forward(x)
            m.stride = torch.tensor([s / x.shape[-2] for x in forward(torch.zeros(1, ch, s, s))])  # forward
            check_anchor_order(m)
            m.anchors /= m.stride.view(-1, 1, 1)
            self.stride = m.stride
            self._initialize_biases()  # only run once

        # Init weights, biases   åˆå§‹åŒ–æƒé‡å’Œåç½®
        initialize_weights(self)
        self.info()
        LOGGER.info("")            #DetectionModel ç±»ç”¨äºåˆå§‹åŒ–å’Œæ„å»º YOLOv5 æ£€æµ‹æ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œé…ç½®å’Œåˆå§‹åŒ–ã€‚

    def forward(self, x, augment=False, profile=False, visualize=False):
        #è¿™ä¸ª forward æ–¹æ³•ç”¨äºæ‰§è¡Œå•å°ºåº¦æˆ–å¢å¼ºæ¨ç†ï¼Œå¹¶å¯ä»¥åŒ…æ‹¬æ€§èƒ½åˆ†ææˆ–å¯è§†åŒ–
        #augmentï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦è¿›è¡Œå¢å¼ºæ¨ç†ï¼ˆé»˜è®¤ä¸º Falseï¼‰ã€‚
        """Performs single-scale or augmented inference and may include profiling or visualization."""
        if augment:   #å¦‚æœ augment ä¸ºçœŸï¼Œåˆ™è°ƒç”¨ _forward_augment æ–¹æ³•æ‰§è¡Œå¢å¼ºæ¨ç†ï¼Œå¹¶è¿”å›ç»“æœã€‚è¿™ä¸ªæ–¹æ³•å¯èƒ½æ˜¯ç±»ä¸­çš„å¦ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºæ‰§è¡Œå¢å¼ºæ¨ç†ã€‚
            return self._forward_augment(x)  # augmented inference, None
        return self._forward_once(x, profile, visualize)  # single-scale inference, train
        #å¦‚æœ augment ä¸ºå‡ï¼Œåˆ™è°ƒç”¨ _forward_once æ–¹æ³•æ‰§è¡Œå•å°ºåº¦æ¨ç†ï¼ŒåŒæ—¶ä¼ é€’ profile å’Œ visualize å‚æ•°ï¼Œå¹¶è¿”å›ç»“æœã€‚è¿™ä¸ªæ–¹æ³•å¯èƒ½æ˜¯ç±»ä¸­çš„å¦ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºæ‰§è¡Œå•å°ºåº¦æ¨ç†å’Œè®­ç»ƒã€‚
    def _forward_augment(self, x):
        """Performs augmented inference across different scales and flips, returning combined detections."""
        img_size = x.shape[-2:]  # height, width
        s = [1, 0.83, 0.67]  # scales   s åŒ…å«äº†ä¸‰ä¸ªå°ºåº¦ç³»æ•°ï¼š1ã€0.83 å’Œ 0.67ã€‚
        f = [None, 3, None]  # flips (2-ud, 3-lr)  f åŒ…å«äº†å¯¹åº”çš„ç¿»è½¬æ–¹å¼ï¼šNoneï¼ˆæ— ç¿»è½¬ï¼‰ã€3ï¼ˆå·¦å³ç¿»è½¬ï¼‰å’Œ Noneï¼ˆæ— ç¿»è½¬ï¼‰ã€‚
        y = []  # outputs   ç©ºåˆ—è¡¨ç”¨äºè¾“å‡º
        for si, fi in zip(s, f):    #æ¥ä¸‹æ¥ï¼Œå¯¹äºæ¯ä¸ªå°ºåº¦ç³»æ•° si å’Œå¯¹åº”çš„ç¿»è½¬æ–¹å¼ fi
            xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))
            #å¦‚æœæœ‰ç¿»è½¬ï¼Œå°±å¯¹è¾“å…¥æ•°æ®è¿›è¡Œç¿»è½¬æ“ä½œï¼Œä½¿ç”¨ flip æ–¹æ³•ï¼Œå°†å·¦å³ç¿»è½¬çš„æƒ…å†µç”¨ç¿»è½¬å‚æ•° fi è¡¨ç¤º
            #å¯¹è¾“å…¥æ•°æ®è¿›è¡Œå°ºåº¦å˜æ¢ï¼Œä½¿ç”¨ scale_img æ–¹æ³•å°†å›¾åƒç¼©æ”¾åˆ°ç›®æ ‡å°ºåº¦ï¼Œå¹¶æ ¹æ®æ¨¡å‹çš„æœ€å¤§æ­¥é•¿è¿›è¡Œç½‘æ ¼æŠ½æ ·ï¼Œå…¶ä¸­ gs=int(self.stride.max()) è¡¨ç¤ºä½¿ç”¨æ¨¡å‹ä¸­æœ€å¤§æ­¥é•¿æ¥è®¾ç½®æŠ½æ ·é—´éš”ã€‚
            yi = self._forward_once(xi)[0]  # forward
            #å¯¹ç¼©æ”¾åçš„è¾“å…¥æ•°æ®è¿›è¡Œå•å°ºåº¦æ¨ç†ï¼Œè°ƒç”¨ _forward_once æ–¹æ³•ï¼Œå¹¶å–å…¶è¿”å›çš„ç»“æœçš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆé€šå¸¸æ˜¯é¢„æµ‹ç»“æœï¼‰
            # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save
            yi = self._descale_pred(yi, fi, si, img_size) #å¯¹é¢„æµ‹ç»“æœè¿›è¡Œåç¼©æ”¾æ“ä½œï¼Œä½¿ç”¨ _descale_pred æ–¹æ³•ï¼Œå°†é¢„æµ‹ç»“æœæ˜ å°„å›åŸå§‹å›¾åƒå°ºå¯¸ã€‚
            y.append(yi)
        y = self._clip_augmented(y)  # clip augmented tails
        #æœ€åï¼Œå¯¹å¢å¼ºåçš„é¢„æµ‹ç»“æœåˆ—è¡¨è¿›è¡Œå¤„ç†ï¼Œä½¿ç”¨ _clip_augmented æ–¹æ³•å‰ªè£å¤šä½™çš„éƒ¨åˆ†ï¼Œä¿ç•™æœ‰æ•ˆçš„æ£€æµ‹ç»“æœã€‚
        return torch.cat(y, 1), None  # augmented inference, train
        #è¿™ä¸ªæ–¹æ³•ç”¨äºæ‰§è¡Œå¢å¼ºæ¨ç†ï¼ŒåŒ…æ‹¬åœ¨ä¸åŒå°ºåº¦å’Œç¿»è½¬æƒ…å†µä¸‹å¯¹è¾“å…¥å›¾åƒè¿›è¡Œå¤„ç†ï¼Œå¹¶å°†å¤šä¸ªå°ºåº¦çš„é¢„æµ‹ç»“æœç»„åˆåœ¨ä¸€èµ·ã€‚

    def _descale_pred(self, p, flips, scale, img_size):
        #è¿™ä¸ª _descale_pred æ–¹æ³•ç”¨äºå°†å¢å¼ºæ¨ç†åçš„é¢„æµ‹ç»“æœè¿›è¡Œåç¼©æ”¾ï¼Œæ ¹æ®ç¿»è½¬å’Œå›¾åƒå°ºå¯¸è¿›è¡Œè°ƒæ•´ã€‚
        #pï¼šé¢„æµ‹ç»“æœã€‚lipsï¼šç¿»è½¬æ–¹å¼ã€‚scaleï¼šå°ºåº¦ç³»æ•°ã€‚img_sizeï¼šå›¾åƒå°ºå¯¸ï¼Œæ ¼å¼ä¸º (height, width)ã€‚
        """De-scales predictions from augmented inference, adjusting for flips and image size."""
        if self.inplace:  #å¦‚æœæ¨¡å‹è®¾ç½®ä¸ºåŸåœ°æ“ä½œï¼ˆself.inplace ä¸ºçœŸï¼‰ï¼Œåˆ™ç›´æ¥å¯¹é¢„æµ‹ç»“æœçš„åæ ‡éƒ¨åˆ†è¿›è¡Œåç¼©æ”¾ï¼š
            p[..., :4] /= scale  # de-scale å°†é¢„æµ‹ç»“æœçš„å‰å››ä¸ªå…ƒç´ ï¼ˆåæ ‡ä¿¡æ¯ï¼‰é™¤ä»¥å°ºåº¦ç³»æ•° scaleï¼Œä»¥è¿›è¡Œåç¼©æ”¾ã€‚
            if flips == 2:
                p[..., 1] = img_size[0] - p[..., 1]  # de-flip ud
                #å¦‚æœç¿»è½¬æ–¹å¼ä¸ºä¸Šä¸‹ç¿»è½¬ï¼ˆflips == 2ï¼‰ï¼Œåˆ™å°†é¢„æµ‹ç»“æœä¸­çš„ y åæ ‡å–åï¼Œå³ img_size[0] - p[..., 1]ã€‚
            elif flips == 3:
                p[..., 0] = img_size[1] - p[..., 0]  # de-flip lr
                #å¦‚æœç¿»è½¬æ–¹å¼ä¸ºå·¦å³ç¿»è½¬ï¼ˆflips == 3ï¼‰ï¼Œåˆ™å°†é¢„æµ‹ç»“æœä¸­çš„ x åæ ‡å–åï¼Œå³ img_size[1] - p[..., 0]ã€‚
        else:   #å¦‚æœæ¨¡å‹ä¸æ˜¯åŸåœ°æ“ä½œï¼Œåˆ™åˆ›å»ºæ–°çš„å¼ é‡æ¥å­˜å‚¨åç¼©æ”¾åçš„ç»“æœï¼š
            x, y, wh = p[..., 0:1] / scale, p[..., 1:2] / scale, p[..., 2:4] / scale  # de-scale
            #å°†é¢„æµ‹ç»“æœçš„ xã€y åæ ‡å’Œå®½é«˜ä¿¡æ¯é™¤ä»¥å°ºåº¦ç³»æ•° scaleï¼Œä»¥è¿›è¡Œåç¼©æ”¾ã€‚
            if flips == 2:
                y = img_size[0] - y  # de-flip ud
            elif flips == 3:
                x = img_size[1] - x  # de-flip lr
            p = torch.cat((x, y, wh, p[..., 4:]), -1)#å°†å¤„ç†åçš„åæ ‡ä¿¡æ¯å’ŒåŸå§‹çš„ç±»åˆ«ç½®ä¿¡åº¦ä¿¡æ¯æ‹¼æ¥èµ·æ¥ã€‚
        return p
            #è¿™ä¸ªæ–¹æ³•ç”¨äºæ ¹æ®è¾“å…¥çš„ç¿»è½¬æ–¹å¼å’Œå›¾åƒå°ºå¯¸å¯¹é¢„æµ‹ç»“æœè¿›è¡Œåç¼©æ”¾ï¼Œå°†å…¶æ˜ å°„å›åŸå§‹å›¾åƒå°ºå¯¸ã€‚

    def _clip_augmented(self, y): #è¿™ä¸ªæ–¹æ³•ç”¨äºä¿®å‰ªå¢å¼ºæ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å¤šä½™éƒ¨åˆ†ï¼Œç¡®ä¿è¾“å‡ºç»“æœçš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚
        #è¿™ä¸ª _clip_augmented æ–¹æ³•ç”¨äºä¿®å‰ªå¢å¼ºæ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿçš„å¤šä½™éƒ¨åˆ†ï¼Œç‰¹åˆ«æ˜¯å¯¹äº YOLOv5 æ¨¡å‹ï¼Œå½±å“ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªå¼ é‡ï¼ŒåŸºäºç½‘æ ¼ç‚¹å’Œå±‚æ¬¡è®¡æ•°ã€‚
        #æ–¹æ³•æ¥å—ä¸€ä¸ªå‚æ•° yï¼Œè¡¨ç¤ºå¢å¼ºæ¨ç†çš„è¾“å‡ºç»“æœ
        """Clips augmented inference tails for YOLOv5 models, affecting first and last tensors based on grid points and
        layer counts.
        """
        nl = self.model[-1].nl  # number of detection layers (P3-P5)
        #é¦–å…ˆï¼Œè·å–æ¨¡å‹ä¸­æœ€åä¸€ä¸ªæ£€æµ‹å±‚çš„æ•°é‡ nlï¼Œå³ P3-P5 å±‚çš„æ•°é‡ã€‚
        g = sum(4**x for x in range(nl))  # grid points
        #æ¥ä¸‹æ¥ï¼Œè®¡ç®—ç½‘æ ¼ç‚¹æ•° gï¼Œé€šè¿‡å¯¹ 4 çš„å¹‚çº§æ•°æ±‚å’Œï¼ŒèŒƒå›´ä» 0 åˆ° nl - 1ã€‚
        e = 1  # exclude layer count   è®¾ç½®ä¸€ä¸ªå˜é‡ eï¼Œç”¨äºæŒ‡å®šè¦æ’é™¤çš„å±‚æ¬¡è®¡æ•°
        i = (y[0].shape[1] // g) * sum(4**x for x in range(e))  # indices
        #è®¡ç®—è¦ä¿ç•™çš„ç´¢å¼• iï¼Œé€šè¿‡å°†å¼ é‡çš„åˆ—æ•°é™¤ä»¥ç½‘æ ¼ç‚¹æ•° gï¼Œå†ä¹˜ä»¥ 4 çš„å¹‚çº§æ•°æ±‚å’Œï¼ŒèŒƒå›´ä» 0 åˆ° e - 1
        y[0] = y[0][:, :-i]  # large   ä½¿ç”¨è®¡ç®—å‡ºçš„ç´¢å¼•ï¼Œä¿®å‰ªå¼ é‡çš„åˆ—ï¼Œå³ä¿ç•™å‰ :-i åˆ—ã€‚
        i = (y[-1].shape[1] // g) * sum(4 ** (nl - 1 - x) for x in range(e))  # indices
        #è®¡ç®—è¦ä¿ç•™çš„ç´¢å¼• iï¼Œé€šè¿‡å°†å¼ é‡çš„åˆ—æ•°é™¤ä»¥ç½‘æ ¼ç‚¹æ•° gï¼Œå†ä¹˜ä»¥ä» nl - 1 - e åˆ° nl - 1 çš„ 4 çš„å¹‚çº§æ•°æ±‚å’Œã€‚
        y[-1] = y[-1][:, i:]  # small    ä½¿ç”¨è®¡ç®—å‡ºçš„ç´¢å¼•ï¼Œä¿®å‰ªå¼ é‡çš„åˆ—ï¼Œå³ä¿ç•™ä»ç¬¬ i åˆ—å¼€å§‹åˆ°æœ«å°¾çš„æ‰€æœ‰åˆ—
        return y

    def _initialize_biases(self, cf=None):
        #è¿™ä¸ª _initialize_biases æ–¹æ³•ç”¨äºä¸º YOLOv5 çš„ Detect() æ¨¡å—åˆå§‹åŒ–åç½®é¡¹ï¼Œå¹¶å¯é€‰æ‹©ä½¿ç”¨ç±»åˆ«é¢‘ç‡ï¼ˆcfï¼‰ï¼Œæ¥å—ä¸€ä¸ªå‚æ•° cfï¼Œè¡¨ç¤ºç±»åˆ«é¢‘ç‡ï¼ˆå¯é€‰ï¼‰
        """
        Initializes biases for YOLOv5's Detect() module, optionally using class frequencies (cf).

        For details see https://arxiv.org/abs/1708.02002 section 3.3.
        """
        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1.
        m = self.model[-1]  # Detect() module  é¦–å…ˆï¼Œè·å–æ¨¡å‹ä¸­æœ€åä¸€ä¸ªæ¨¡å— mï¼Œå³ Detect() æ¨¡å—ã€‚
        #å¯¹äºæ¯ä¸ªæ¨¡å— mi å’Œå¯¹åº”çš„æ­¥é•¿ sï¼Œè¿›è¡Œä»¥ä¸‹æ“ä½œ
        for mi, s in zip(m.m, m.stride):  # from
            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)
            #å°†å½“å‰æ¨¡å—çš„åç½®é¡¹ mi.bias é‡æ–°å½¢çŠ¶ä¸º (na, -1)ï¼Œå…¶ä¸­ na æ˜¯é”šæ¡†çš„æ•°é‡
            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)
            #å°†åç½®é¡¹çš„ç¬¬ 4 åˆ—ï¼ˆç´¢å¼•ä» 0 å¼€å§‹ï¼‰åŠ ä¸Šä¸€ä¸ªåç½®å€¼ï¼Œè¯¥åç½®å€¼æ˜¯æ ¹æ®å¯¹è±¡æ•°é‡å’Œå›¾åƒå°ºå¯¸è®¡ç®—å¾—å‡ºçš„ã€‚
            b.data[:, 5 : 5 + m.nc] += (
                math.log(0.6 / (m.nc - 0.99999)) if cf is None else torch.log(cf / cf.sum())
            )  # cls   å°†åç½®é¡¹çš„ç¬¬ 5 åˆ°ç¬¬ 5 + m.nc åˆ—ï¼ˆç´¢å¼•ä» 0 å¼€å§‹ï¼‰åŠ ä¸Šå¦ä¸€ä¸ªåç½®å€¼ï¼Œè¯¥åç½®å€¼æ˜¯æ ¹æ®ç±»åˆ«æ•°é‡å’Œç±»åˆ«é¢‘ç‡ï¼ˆå¦‚æœæä¾›äº†ï¼‰è®¡ç®—å¾—å‡ºçš„
            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)
            #æœ€åå°†ç»è¿‡è°ƒæ•´çš„åç½®é¡¹é‡æ–°èµ‹å€¼ç»™æ¨¡å—çš„åç½®å‚æ•°ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºå¯è®­ç»ƒçŠ¶æ€

Model = DetectionModel  # retain YOLOv5 'Model' class for backwards compatibility
#è¿™ä¸ªæ–¹æ³•ç”¨äºæ ¹æ®å¯¹è±¡æ•°é‡ã€ç±»åˆ«æ•°é‡ã€å›¾åƒå°ºå¯¸å’Œç±»åˆ«é¢‘ç‡ï¼ˆå¯é€‰ï¼‰æ¥åˆå§‹åŒ– Detect() æ¨¡å—çš„åç½®é¡¹ï¼Œä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°é€‚åº”è®­ç»ƒæ•°æ®å’Œä»»åŠ¡è¦æ±‚ã€‚


class SegmentationModel(DetectionModel): #ç”¨äºåˆå§‹åŒ– YOLOv5 åˆ†å‰²æ¨¡å‹
    # YOLOv5 segmentation model    è°ƒç”¨äº†çˆ¶ç±» DetectionModel çš„æ„é€ å‡½æ•°,å°†å‚æ•°ä¼ é€’ç»™çˆ¶ç±»æ¥åˆå§‹åŒ–åˆ†å‰²æ¨¡å‹ã€‚
    def __init__(self, cfg="yolov5s-seg.yaml", ch=3, nc=None, anchors=None):
        #cfgï¼šé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º "yolov5s-seg.yaml"ã€‚ncï¼šç±»åˆ«æ•°ï¼Œé»˜è®¤ä¸º Noneã€‚
        """Initializes a YOLOv5 segmentation model with configurable params: cfg (str) for configuration, ch (int) for channels, nc (int) for num classes, anchors (list)."""
        super().__init__(cfg, ch, nc, anchors)


class ClassificationModel(BaseModel): #ç”¨äºåˆå§‹åŒ– YOLOv5 åˆ†ç±»æ¨¡å‹
    # YOLOv5 classification model    ClassificationModel ç±»ç”¨äºæ ¹æ®ç»™å®šçš„æ£€æµ‹æ¨¡å‹æˆ–é…ç½®æ–‡ä»¶åˆå§‹åŒ– YOLOv5 åˆ†ç±»æ¨¡å‹ï¼Œå¹¶æä¾›äº†çµæ´»çš„å‚æ•°é…ç½®é€‰é¡¹ã€‚
    def __init__(self, cfg=None, model=None, nc=1000, cutoff=10):#cutoffï¼šæˆªæ–­ç´¢å¼•ï¼Œé»˜è®¤ä¸º 10ã€‚
        """Initializes YOLOv5 model with config file `cfg`, input channels `ch`, number of classes `nc`, and `cuttoff`
        index.
        """
        super().__init__()
        self._from_detection_model(model, nc, cutoff) if model is not None else self._from_yaml(cfg)
        #å¦‚æœæä¾›äº†å·²æœ‰çš„æ£€æµ‹æ¨¡å‹ modelï¼Œåˆ™è°ƒç”¨ _from_detection_model æ–¹æ³•æ¥åˆå§‹åŒ–åˆ†ç±»æ¨¡å‹ï¼Œå¹¶ä¼ é€’ç»™å®šçš„ç±»åˆ«æ•°å’Œæˆªæ–­ç´¢å¼•
        #å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶è·¯å¾„ cfgï¼Œåˆ™è°ƒç”¨ _from_yaml æ–¹æ³•æ¥ä»é…ç½®æ–‡ä»¶ä¸­åŠ è½½æ¨¡å‹çš„é…ç½®ä¿¡æ¯ï¼Œå¹¶åˆå§‹åŒ–åˆ†ç±»æ¨¡å‹ã€‚
    def _from_detection_model(self, model, nc=1000, cutoff=10):
        #è¿™ä¸ª _from_detection_model æ–¹æ³•ç”¨äºä»ä¸€ä¸ª YOLOv5 æ£€æµ‹æ¨¡å‹åˆ›å»ºä¸€ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œé€šè¿‡æŒ‡å®šçš„æˆªæ–­ç´¢å¼• cutoff åˆ‡ç‰‡æ¨¡å‹ï¼Œå¹¶æ·»åŠ ä¸€ä¸ªåˆ†ç±»å±‚ã€‚
        """Creates a classification model from a YOLOv5 detection model, slicing at `cutoff` and adding a classification
        layer.
        """
        if isinstance(model, DetectMultiBackend): #å¦‚æœ model æ˜¯ DetectMultiBackend ç±»çš„å®ä¾‹ï¼Œåˆ™å°†å…¶è§£åŒ…ä»¥è·å–å†…éƒ¨çš„æ£€æµ‹æ¨¡å‹
            model = model.model  # unwrap DetectMultiBackend
        model.model = model.model[:cutoff]  # backbone
        #ä» model ä¸­æå–å‡ºè¦ä¿ç•™çš„éƒ¨åˆ†ï¼Œå³ model.model[:cutoff]ï¼Œå³æˆªå–åˆ°æŒ‡å®šçš„æˆªæ–­ç´¢å¼•å¤„ï¼Œä½œä¸ºåˆ†ç±»æ¨¡å‹çš„ä¸»å¹²éª¨æ¶ã€‚
        m = model.model[-1]  # last layer   è·å–ä¸»å¹²éª¨æ¶çš„æœ€åä¸€å±‚ï¼Œå¹¶æå–å…¶è¾“å…¥é€šé“æ•° chã€‚
        ch = m.conv.in_channels if hasattr(m, "conv") else m.cv1.conv.in_channels  # ch into module
        c = Classify(ch, nc)  # Classify() åˆ›å»ºä¸€ä¸ª Classify å®ä¾‹ cï¼Œç”¨äºåˆ†ç±»ï¼Œå…¶ä¸­ä¼ å…¥å‚æ•°æ˜¯è¾“å…¥é€šé“æ•° ch å’Œç±»åˆ«æ•° nc
        c.i, c.f, c.type = m.i, m.f, "models.common.Classify"  # index, from, typeè®¾ç½®åˆ†ç±»å±‚çš„ç´¢å¼•ã€æ¥æºå’Œç±»å‹ã€‚
        model.model[-1] = c  # replace å°†åˆ†ç±»å±‚æ›¿æ¢ä¸ºåŸæ¨¡å‹çš„æœ€åä¸€å±‚ï¼Œå¹¶æ›´æ–°åˆ†ç±»æ¨¡å‹çš„å±æ€§ï¼ˆå¦‚ä¸»å¹²éª¨æ¶ã€æ­¥é•¿ã€ä¿å­˜åˆ—è¡¨å’Œç±»åˆ«æ•°ï¼‰
        self.model = model.model
        self.stride = model.stride
        self.save = []
        self.nc = nc
        #è¿™ä¸ªæ–¹æ³•ç”¨äºä»ä¸€ä¸ª YOLOv5 æ£€æµ‹æ¨¡å‹åˆ›å»ºä¸€ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œé€šè¿‡åˆ‡ç‰‡æˆªæ–­æ¨¡å‹å¹¶æ·»åŠ ä¸€ä¸ªåˆ†ç±»å±‚æ¥å®Œæˆè½¬æ¢ã€‚

    def _from_yaml(self, cfg):#è¿™ä¸ª _from_yaml æ–¹æ³•ç”¨äºä»æŒ‡å®šçš„ YAML é…ç½®æ–‡ä»¶åˆ›å»ºä¸€ä¸ª YOLOv5 åˆ†ç±»æ¨¡å‹
        """Creates a YOLOv5 classification model from a specified *.yaml configuration file."""
        self.model = None   #è®¾ç½®äº† self.model ä¸º Noneï¼Œè¡¨ç¤ºæš‚æ—¶ä¸ä»é…ç½®æ–‡ä»¶ä¸­åˆ›å»ºæ¨¡å‹ã€‚
        #è¿™ä¸ªæ–¹æ³•çš„å®ç°å¯èƒ½æ˜¯ç”±äºåœ¨ YOLOv5 åˆ†ç±»æ¨¡å‹ä¸­ï¼Œé€šå¸¸ä¼šåœ¨æ„é€ å‡½æ•°ä¸­ä½¿ç”¨é…ç½®æ–‡ä»¶æ¥åˆå§‹åŒ–æ¨¡å‹ï¼Œè€Œä¸æ˜¯åœ¨è¿™ä¸ªæ–¹æ³•ä¸­ã€‚

def parse_model(d, ch):
    #è¿™ä¸ª parse_model å‡½æ•°ç”¨äºè§£æä¸€ä¸ª YOLOv5 æ¨¡å‹çš„é…ç½®å­—å…¸ dï¼Œæ ¹æ®è¾“å…¥é€šé“ ch å’Œæ¨¡å‹æ¶æ„é…ç½®æ¨¡å‹çš„å±‚æ¬¡ç»“æ„
    """Parses a YOLOv5 model from a dict `d`, configuring layers based on input channels `ch` and model architecture."""
    LOGGER.info(f"\n{'':>3}{'from':>18}{'n':>3}{'params':>10}  {'module':<40}{'arguments':<30}")
    #è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯æ‰“å°ä¸€ä¸ªåŒ…å«æ¨¡å‹è¯¦ç»†ä¿¡æ¯çš„è¡¨å¤´ï¼Œç”¨äºåœ¨æ—¥å¿—ä¸­å±•ç¤ºæ¯ä¸ªæ¨¡å—çš„ç›¸å…³ä¿¡æ¯
    anchors, nc, gd, gw, act, ch_mul = (
        d["anchors"],
        d["nc"],
        d["depth_multiple"],
        d["width_multiple"],
        d.get("activation"),
        d.get("channel_multiple"),
    )
    if act:
        Conv.default_act = eval(act)  # redefine default activation, i.e. Conv.default_act = nn.SiLU()
        LOGGER.info(f"{colorstr('activation:')} {act}")  # print
        #å¦‚æœæ¿€æ´»å‡½æ•° act å­˜åœ¨ï¼ˆå³ä¸ä¸º Noneï¼‰ï¼Œåˆ™é‡æ–°å®šä¹‰äº†é»˜è®¤çš„å·ç§¯å±‚æ¿€æ´»å‡½æ•° Conv.default_act ä¸ºç»™å®šçš„æ¿€æ´»å‡½æ•°ã€‚ä½¿ç”¨ eval(act) æ¥å°†å­—ç¬¦ä¸²å½¢å¼çš„æ¿€æ´»å‡½æ•°åè½¬æ¢ä¸ºå®é™…çš„å‡½æ•°å¯¹è±¡ã€‚ç„¶åé€šè¿‡ LOGGER.info() æ‰“å°äº†æ¿€æ´»å‡½æ•°çš„ä¿¡æ¯ã€‚è¿™ä¸€æ­¥æ˜¯ä¸ºäº†æ ¹æ®é…ç½®æ–‡ä»¶ä¸­æä¾›çš„æ¿€æ´»å‡½æ•°ç±»å‹é‡æ–°å®šä¹‰é»˜è®¤æ¿€æ´»å‡½æ•°ã€‚
    if not ch_mul:
        ch_mul = 8
        #å¦‚æœé€šé“å€æ•° ch_mul ä¸å­˜åœ¨æˆ–ä¸º 0ï¼Œåˆ™å°†å…¶è®¾ä¸ºé»˜è®¤å€¼ 8ã€‚é€šé“å€æ•°ç”¨äºè°ƒæ•´æ¨¡å‹çš„é€šé“æ•°é‡ï¼Œä»¥å¢åŠ æˆ–å‡å°‘æ¨¡å‹çš„å¤æ‚åº¦
    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors
    #è®¡ç®—äº†é”šæ¡†çš„æ•°é‡ naã€‚å¦‚æœ anchors æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ™è¡¨ç¤ºå…·æœ‰å¤šä¸ªå°ºåº¦çš„é”šæ¡†ï¼Œè¿™é‡Œå–ç¬¬ä¸€ä¸ªå°ºåº¦çš„é”šæ¡†æ•°é‡ã€‚å¦‚æœ anchors æ˜¯ä¸€ä¸ªæ•°å€¼ï¼Œåˆ™è¡¨ç¤ºé”šæ¡†çš„æ•°é‡ã€‚é”šæ¡†ç”¨äºç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸­çš„é¢„æµ‹ã€‚
    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)
    #è®¡ç®—äº†æ¨¡å‹è¾“å‡ºçš„é€šé“æ•° noã€‚æ¨¡å‹çš„è¾“å‡ºé€šé“æ•°ç­‰äºé”šæ¡†æ•°é‡ä¹˜ä»¥ï¼ˆç±»åˆ«æ•°åŠ ä¸Š 5ï¼‰ï¼Œå…¶ä¸­ 5 è¡¨ç¤ºç›®æ ‡çš„åæ ‡ä¿¡æ¯ï¼ˆä¸­å¿ƒç‚¹åæ ‡ã€å®½åº¦ã€é«˜åº¦ï¼‰ä»¥åŠç½®ä¿¡åº¦åˆ†æ•°

    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out
    #layers ç”¨äºå­˜å‚¨æ¨¡å‹çš„æ¯ä¸€å±‚ï¼Œsave ç”¨äºå­˜å‚¨éœ€è¦ä¿å­˜è¾“å‡ºçš„å±‚æ¬¡ç´¢å¼•ï¼Œc2 åˆ™åˆå§‹åŒ–ä¸ºè¾“å…¥é€šé“åˆ—è¡¨ ch çš„æœ€åä¸€ä¸ªå€¼ï¼Œå³æ¨¡å‹çš„è¾“å‡ºé€šé“æ•°
    for i, (f, n, m, args) in enumerate(d["backbone"] + d["head"]):  # from, number, module, args
        #ä½¿ç”¨ enumerate() éå†äº†é…ç½®å­—å…¸ä¸­çš„ "backbone" å’Œ "head" éƒ¨åˆ†ï¼Œå…¶ä¸­ "backbone" è¡¨ç¤ºæ¨¡å‹çš„ä¸»å¹²ç½‘ç»œéƒ¨åˆ†ï¼Œ"head" è¡¨ç¤ºæ¨¡å‹çš„å¤´éƒ¨ç½‘ç»œéƒ¨åˆ†ã€‚å¯¹äºæ¯ä¸ªéƒ¨åˆ†ï¼Œæå–äº†æ¥æº fã€æ•°é‡ nã€æ¨¡å—ç±»å‹ m å’Œå‚æ•° argsã€‚
        m = eval(m) if isinstance(m, str) else m  # eval strings
        #å¯¹äºæ¨¡å—ç±»å‹ mï¼Œå¦‚æœæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œåˆ™ä½¿ç”¨ eval() å‡½æ•°å°†å…¶è½¬æ¢ä¸ºå®é™…çš„ç±»å¯¹è±¡ï¼›ç„¶åéå†å‚æ•°åˆ—è¡¨ argsï¼Œå¦‚æœå‚æ•°æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œåˆ™åŒæ ·ä½¿ç”¨ eval() å‡½æ•°å°†å…¶è½¬æ¢ä¸ºå®é™…çš„å€¼ã€‚è¿™ä¸€æ­¥æ˜¯ä¸ºäº†ç¡®ä¿æ¨¡å—ç±»å‹å’Œå‚æ•°çš„æ­£ç¡®æ€§ï¼Œå¹¶å°†å­—ç¬¦ä¸²å½¢å¼çš„å‚æ•°è½¬æ¢ä¸ºå®é™…çš„å¯¹è±¡ã€‚
        for j, a in enumerate(args):
            with contextlib.suppress(NameError):
                args[j] = eval(a) if isinstance(a, str) else a  # eval strings

        n = n_ = max(round(n * gd), 1) if n > 1 else n  # depth gain
        #è®¡ç®—æ·±åº¦å¢ç›Š n_ï¼Œå…¶ä¸­ gd æ˜¯æ·±åº¦å€æ•°ï¼Œè¡¨ç¤ºäº†æ¨¡å‹çš„æ·±åº¦ç›¸å¯¹äºåŸå§‹è®¾è®¡çš„å¢ç›Šã€‚å¦‚æœè¾“å…¥çš„ n å¤§äº 1ï¼ˆå³æœ‰å¤šä¸ªé‡å¤æ¨¡å—ï¼‰ï¼Œåˆ™å°†å…¶ä¹˜ä»¥æ·±åº¦å€æ•° gd å¹¶å‘ä¸Šå–æ•´ï¼Œå¦åˆ™æ·±åº¦å¢ç›Šä¸º 1ã€‚è¿™ä¸€æ­¥æ˜¯ä¸ºäº†æ ¹æ®é…ç½®æ–‡ä»¶ä¸­æä¾›çš„æ·±åº¦å€æ•°è°ƒæ•´æ¨¡å‹çš„æ·±åº¦ã€‚
        if m in {
            Conv,
            GhostConv,
            Bottleneck,
            GhostBottleneck,
            SPP,
            SPPF,
            DWConv,
            MixConv2d,
            Focus,
            CrossConv,
            BottleneckCSP,
            C3,
            C3TR,
            C3SPP,
            C3Ghost,
            nn.ConvTranspose2d,
            DWConvTranspose2d,
            C3x,
            SE_Block,
            SEAttention,
            FEM,
            PSA,
            SPPELAN,

            SPPFCSPC,
            MultiSpectralAttentionLayer,
            FEMS,
            EMA,

            FFM_Concat3,
            C2fCIBAttention,
            SelfAttention,
            SE,
            G_bneck,
            C3_DCN,
            CoordAtt,
            C3_CA,
            CSPPC,
            C2fGhost
        }:
            c1, c2 = ch[f], args[0]
            if c2 != no:  # if not output
                c2 = make_divisible(c2 * gw, ch_mul)
                #æå–è¾“å…¥é€šé“æ•° c1 å’Œè¾“å‡ºé€šé“æ•° c2ã€‚è¾“å…¥é€šé“æ•° c1 å–è‡ªè¾“å…¥é€šé“åˆ—è¡¨ ch çš„ç´¢å¼• fï¼Œè¾“å‡ºé€šé“æ•° c2 å–è‡ªå‚æ•°åˆ—è¡¨ args çš„ç¬¬ä¸€ä¸ªå‚æ•°
                #å¦‚æœè¾“å‡ºé€šé“æ•° c2 ä¸ç­‰äºæ¨¡å‹çš„æ€»è¾“å‡ºé€šé“æ•° noï¼Œåˆ™å°†å…¶ä¹˜ä»¥å®½åº¦å€æ•° gw å¹¶è°ƒç”¨ make_divisible å‡½æ•°ï¼Œç¡®ä¿è¾“å‡ºé€šé“æ•°æ˜¯å¯ä»¥è¢«é€šé“å€æ•° ch_mul æ•´é™¤çš„ã€‚è¿™ä¸€æ­¥æ˜¯ä¸ºäº†æ ¹æ®é…ç½®æ–‡ä»¶ä¸­æä¾›çš„å®½åº¦å€æ•°è°ƒæ•´æ¨¡å‹çš„å®½åº¦ï¼Œå¹¶ç¡®ä¿è¾“å‡ºé€šé“æ•°æ˜¯åˆç†çš„ã€‚
            args = [c1, c2, *args[1:]]
            if m in {BottleneckCSP, C3, C3TR, C3Ghost, C3x}:
                args.insert(2, n)  # number of repeats
                n = 1
    #å¦‚æœæ¨¡å—ç±»å‹æ˜¯ BottleneckCSPã€C3ã€C3TRã€C3Ghost æˆ– C3x ä¸­çš„ä¸€ç§ï¼Œåˆ™åœ¨å‚æ•°åˆ—è¡¨ args çš„ç¬¬ä¸‰ä¸ªä½ç½®ï¼ˆç´¢å¼•ä¸º 2ï¼‰æ’å…¥é‡å¤æ¬¡æ•° nï¼Œç„¶åå°† n è®¾ä¸º 1ã€‚è¿™æ˜¯å› ä¸ºè¿™äº›æ¨¡å—æ˜¯éœ€è¦é‡å¤å¤šæ¬¡çš„æ¨¡å—ï¼Œè€Œé‡å¤æ¬¡æ•°å·²ç»åœ¨ä¹‹å‰çš„æ“ä½œä¸­è¿›è¡Œäº†è°ƒæ•´ï¼Œå› æ­¤éœ€è¦åœ¨å‚æ•°åˆ—è¡¨ä¸­æ’å…¥è¿™ä¸ªå€¼ã€‚
        elif m is nn.BatchNorm2d:
            args = [ch[f]]
        elif m in {MLLAttention}:
            c2=ch[f]
            args=[c2,*args]
        elif m in (DSConv,DSConv_C3):
            c1,c2=ch[f],args[0]
            if c2 !=nc:
                c2=make_divisible(c2* gw,8)
            args=[c1,c2, *args[1:]]
            if m is DSConv_C3:
                args.insert(2,n)
                n=1
    #å¦‚æœæ¨¡å—ç±»å‹æ˜¯ nn.BatchNorm2dï¼Œåˆ™å°†å‚æ•°åˆ—è¡¨ args è®¾ç½®ä¸ºåŒ…å«è¾“å…¥é€šé“æ•° ch[f] çš„åˆ—è¡¨ã€‚è¿™æ˜¯å› ä¸ºæ‰¹é‡å½’ä¸€åŒ–å±‚çš„å‚æ•°åªä¸è¾“å…¥é€šé“æ•°ç›¸å…³ã€‚
        elif m is Concat:
            c2 = sum(ch[x] for x in f)
    #å¦‚æœæ¨¡å—ç±»å‹æ˜¯ Concatï¼Œåˆ™å°†è¾“å‡ºé€šé“æ•° c2 è®¾ç½®ä¸ºè¾“å…¥é€šé“åˆ—è¡¨ ch ä¸­ç´¢å¼•ä¸º f çš„æ‰€æœ‰é€šé“æ•°çš„æ€»å’Œã€‚è¿™æ˜¯å› ä¸ºæ‹¼æ¥æ“ä½œä¼šå°†æ‰€æœ‰è¾“å…¥é€šé“è¿æ¥èµ·æ¥ã€‚
        elif m in [MobileViTv2_Block]:
            c1,c2=ch[f],args[0]
            if c2!=no:
                c2=make_divisible(c2*gw,8)
                args=[c1,c2]
            if m in [MobileViTv2_Block]:
                args.insert(2,n)
                n=1
        elif m in [C2f,C2]:
            c1,c2=ch[f],args[0]
            if c2!=no:
                c2=make_divisible(c2*gw,8)
                args=[c1,c2]
            if m in [C2f,C2]:
                args.insert(2,n)
                n=1
        elif m in {SCAM}:
            c2 = ch[f]
            args = [c2]
        elif m in {SCAMS}:
            c2 = ch[f]
            args = [c2]
        elif m in {SCAMSE}:
            c2 = ch[f]
            args = [c2]
        elif m is LSKblock:
            c1=ch[f]
            args=[c1,*args[0:]]
        elif m is FFM_Concat2:
            c2 = sum(ch[x] for x in f)
            args = [args[0], c2 // 2, c2 // 2]
        elif m is FFM_Concat3:
            c2 = sum(ch[x] for x in f)
            args = [args[0], c2 // 4, c2 // 2, c2 // 4]
        # TODO: channel, gw, gd
        elif m in {Detect, Segment}:
            args.append([ch[x] for x in f])
            if isinstance(args[1], int):  # number of anchors
                args[1] = [list(range(args[1] * 2))] * len(f)
            if m is Segment:
                args[3] = make_divisible(args[3] * gw, ch_mul)
    #å¦‚æœæ¨¡å—ç±»å‹æ˜¯ Detect æˆ– Segmentï¼Œåˆ™å°†è¾“å…¥é€šé“åˆ—è¡¨ ch ä¸­ç´¢å¼•ä¸º f çš„æ‰€æœ‰é€šé“æ•°æ·»åŠ åˆ°å‚æ•°åˆ—è¡¨ args ä¸­ã€‚å¦‚æœå‚æ•°åˆ—è¡¨ä¸­çš„ç¬¬äºŒä¸ªå‚æ•°æ˜¯æ•´æ•°ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºé”šæ¡†åˆ—è¡¨ã€‚å¦‚æœæ¨¡å—ç±»å‹æ˜¯ Segmentï¼Œè¿˜ä¼šæ ¹æ®å®½åº¦å€æ•° gw å’Œé€šé“å€æ•° ch_mul è°ƒæ•´å‚æ•°åˆ—è¡¨ä¸­çš„å€¼ã€‚
        elif m is Contract:
            c2 = ch[f] * args[0] ** 2
    #å¦‚æœæ¨¡å—ç±»å‹æ˜¯ Contractï¼Œåˆ™å°†è¾“å‡ºé€šé“æ•° c2 è®¾ç½®ä¸ºè¾“å…¥é€šé“æ•° ch[f] ä¹˜ä»¥ç¬¬ä¸€ä¸ªå‚æ•°çš„å¹³æ–¹ã€‚
        elif m is Expand:
            c2 = ch[f] // args[0] ** 2
    #å¦‚æœæ¨¡å—ç±»å‹æ˜¯ Expandï¼Œåˆ™å°†è¾“å‡ºé€šé“æ•° c2 è®¾ç½®ä¸ºè¾“å…¥é€šé“æ•° ch[f] é™¤ä»¥ç¬¬ä¸€ä¸ªå‚æ•°çš„å¹³æ–¹ã€‚
        else:
            c2 = ch[f]  #å¯¹äºå…¶ä»–æ¨¡å—ç±»å‹ï¼Œç›´æ¥å°†è¾“å‡ºé€šé“æ•° c2 è®¾ç½®ä¸ºè¾“å…¥é€šé“åˆ—è¡¨ ch ä¸­ç´¢å¼•ä¸º f çš„é€šé“æ•°ã€‚

        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # module
        #æ ¹æ®é‡å¤æ¬¡æ•° nï¼Œä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šä¸ªæ¨¡å— m(*args) çš„ nn.Sequential å®¹å™¨ m_ã€‚å¦‚æœé‡å¤æ¬¡æ•° n å¤§äº 1ï¼Œåˆ™ä½¿ç”¨ nn.Sequential å°†å¤šä¸ªç›¸åŒçš„æ¨¡å—å †å èµ·æ¥ï¼›å¦åˆ™ç›´æ¥ä½¿ç”¨å•ä¸ªæ¨¡å—ã€‚è¿™ä¸€æ­¥æ˜¯ä¸ºäº†æ ¹æ®é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„é‡å¤æ¬¡æ•°æ„å»ºæ¨¡å‹çš„æ·±åº¦
        t = str(m)[8:-2].replace("__main__.", "")  # module type
        np = sum(x.numel() for x in m_.parameters())  # number params
        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params
        LOGGER.info(f"{i:>3}{str(f):>18}{n_:>3}{np:10.0f}  {t:<40}{str(args):<30}")  # print
        #å°†éœ€è¦ä¿å­˜çš„å±‚çš„ç´¢å¼•æ·»åŠ åˆ°ä¿å­˜åˆ—è¡¨ save ä¸­ã€‚å¦‚æœ f æ˜¯æ•´æ•°ï¼Œåˆ™ç›´æ¥å°†å…¶æ·»åŠ åˆ°ä¿å­˜åˆ—è¡¨ä¸­ï¼›å¦‚æœ f æ˜¯åˆ—è¡¨ï¼Œåˆ™éå†å…¶ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œå¦‚æœä¸æ˜¯ -1ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°ä¿å­˜åˆ—è¡¨ä¸­ã€‚è¿™ä¸€æ­¥æ˜¯ä¸ºäº†åç»­å¯¹æ¨¡å‹è¿›è¡Œä¿å­˜æ“ä½œåšå‡†å¤‡ã€‚
        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist
        layers.append(m_)
        if i == 0:
            ch = []
        ch.append(c2) #æœ€åï¼Œå°†æ„å»ºå¥½çš„æ¨¡å‹çš„å±‚åˆ—è¡¨ layers è½¬æ¢ä¸º nn.Sequential å®¹å™¨ï¼Œå¹¶è¿”å›ç»™è°ƒç”¨è€…ã€‚åŒæ—¶ï¼Œä¿å­˜åˆ—è¡¨ save ä¹ŸæŒ‰ç…§ç´¢å¼•æ’åºï¼Œä»¥ç¡®ä¿ä¿å­˜æ“ä½œçš„æ­£ç¡®æ€§ã€‚
    return nn.Sequential(*layers), sorted(save)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--cfg", type=str, default="yolov5s.yaml", help="model.yaml")
    parser.add_argument("--batch-size", type=int, default=1, help="total batch size for all GPUs")
    parser.add_argument("--device", default="", help="cuda device, i.e. 0 or 0,1,2,3 or cpu")
    parser.add_argument("--profile", action="store_true", help="profile model speed")
    parser.add_argument("--line-profile", action="store_true", help="profile model speed layer by layer")
    parser.add_argument("--test", action="store_true", help="test all yolo*.yaml")
    opt = parser.parse_args()
    opt.cfg = check_yaml(opt.cfg)  # check YAML
    print_args(vars(opt))
    device = select_device(opt.device)

    # Create model
    im = torch.rand(opt.batch_size, 3, 640, 640).to(device)
    model = Model(opt.cfg).to(device)

    # Options
    if opt.line_profile:  # profile layer by layer
        model(im, profile=True)

    elif opt.profile:  # profile forward-backward
        results = profile(input=im, ops=[model], n=3)

    elif opt.test:  # test all models
        for cfg in Path(ROOT / "models").rglob("yolo*.yaml"):
            try:
                _ = Model(cfg)
            except Exception as e:
                print(f"Error in {cfg}: {e}")

    else:  # report fused model summary
        model.fuse()
